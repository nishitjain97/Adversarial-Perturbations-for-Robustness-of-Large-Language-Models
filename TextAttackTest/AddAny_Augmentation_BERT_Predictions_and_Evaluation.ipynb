{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishitjain97/NLP_538_Fall_2022_Project_HaND/blob/main/TextAttackTest/AddAny_Augmentation_BERT_Predictions_and_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook for BERT predictions and evaluation"
      ],
      "metadata": {
        "id": "stjaAVAAzrAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Loading"
      ],
      "metadata": {
        "id": "wqkfFSFWzwJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6rZUG9BzIoj",
        "outputId": "f6f160f8-c150-45db-a5f1-c971d75943bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWqwDuIJzOls",
        "outputId": "f9afd568-54fe-4316-b4ee-832afccf96a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import multilabel_confusion_matrix, f1_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "u8xgm7Q5zTfp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "fysV-nU_zzbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentiment(sentiment):\n",
        "  if sentiment == 'Negative':\n",
        "    return 0\n",
        "  elif sentiment == 'Neutral':\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "WkXcU1QhzkRJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_sentence(tokenizer, sentence, min_aug_len=5, max_aug_len=50, max_sent_len=512, position='end'):\n",
        "  original = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "\n",
        "  augmentation_length = random.randint(min_aug_len, max_aug_len)\n",
        "\n",
        "  if len(original) >= max_sent_len - 2 - augmentation_length:\n",
        "    original = original[:(max_sent_len  - 2 - augmentation_length)]\n",
        "    \n",
        "  vals = pd.Series(tokenizer.get_vocab().values())\n",
        "  vals = vals[vals >= 999]\n",
        "\n",
        "  augmentation = [vals.sample(1).values[0] for i in range(augmentation_length)]\n",
        "\n",
        "  if position == 'end':\n",
        "    original = [101] + original + augmentation + [102]\n",
        "  else:\n",
        "    original = [101] + augmentation + original + [102]\n",
        "    \n",
        "  segment_ids = [0] * len(original)\n",
        "  input_mask = [1] * len(original)\n",
        "\n",
        "  while len(original) < max_sent_len:\n",
        "    original.append(0)\n",
        "    segment_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "\n",
        "  original = torch.tensor([original])\n",
        "  segment_ids = torch.tensor([segment_ids])\n",
        "  input_mask = torch.tensor([input_mask])\n",
        "  \n",
        "  encoded_dict = {\n",
        "      'input_ids': original,\n",
        "      'token_type_ids': segment_ids,\n",
        "      'attention_mask': input_mask\n",
        "  }\n",
        "\n",
        "  return encoded_dict\n",
        "\n",
        "def process_data(data, tokenizer, min_aug_len=5, max_aug_len=50, position='end'):\n",
        "  input_ids = {}\n",
        "  attention_masks = {}\n",
        "  labels = {}\n",
        "  \n",
        "  for key in data.keys():\n",
        "    print(key)\n",
        "    input_ids[key] = []\n",
        "    attention_masks[key] = []\n",
        "    \n",
        "    for sentence in data[key]['DOCUMENT']:\n",
        "      encoded_dict = augment_sentence(tokenizer, sentence, min_aug_len, max_aug_len, position=position)\n",
        "      input_ids[key].append(encoded_dict['input_ids'])\n",
        "      attention_masks[key].append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids[key] = torch.cat(input_ids[key], dim=0)\n",
        "    attention_masks[key] = torch.cat(attention_masks[key], dim=0)\n",
        "\n",
        "  for key in data.keys():\n",
        "    labels[key] = data[key]['TRUE_SENTIMENT'].apply(lambda x: encode_sentiment(x))\n",
        "\n",
        "  dataset = {}\n",
        "  \n",
        "  for key in data.keys():\n",
        "    dataset[key] = TensorDataset(input_ids[key], attention_masks[key], torch.tensor(labels[key].tolist()))\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "Lb6RBWpc0a3L"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataloaders(config, dataset):\n",
        "  dataloaders = {}\n",
        "  \n",
        "  for key in dataset.keys():\n",
        "    dataloaders[key] = DataLoader(\n",
        "        dataset[key],\n",
        "        sampler = RandomSampler(dataset[key]),\n",
        "        batch_size = config[key]\n",
        "    )\n",
        "\n",
        "  return dataloaders"
      ],
      "metadata": {
        "id": "fyg5YsX_0vob"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def get_accuracy(preds, labels):\n",
        "  return np.sum(preds == labels) / len(labels)\n",
        "\n",
        "def multiclass_confusion_matrix(preds, labels):\n",
        "  return multilabel_confusion_matrix(labels, preds)"
      ],
      "metadata": {
        "id": "ou7JjGhk0pdH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model, dataloader):\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        result = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask,\n",
        "                      return_dict=True)\n",
        "\n",
        "    logits = result.logits\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    \n",
        "  return predictions, true_labels\n",
        "\n",
        "def get_precision(cm):\n",
        "  true_pos = np.diag(cm) \n",
        "  return np.sum(true_pos / np.sum(cm, axis=0))\n",
        "\n",
        "def get_recall(cm):\n",
        "  true_pos = np.diag(cm)\n",
        "  return np.sum(true_pos / np.sum(cm, axis=1))\n",
        "\n",
        "def evaluate_predictions(predictions, true_labels):\n",
        "  predictions = np.argmax(predictions, axis=1).flatten()\n",
        "  true_labels = true_labels.flatten()\n",
        "  \n",
        "  acc = get_accuracy(predictions, true_labels)\n",
        "  p = precision_score(true_labels, predictions, average=None)\n",
        "  r = recall_score(true_labels, predictions, average=None)\n",
        "  f1 = f1_score(true_labels, predictions, average=None)\n",
        "  macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
        "  \n",
        "  return acc, p, r, f1, macro_f1"
      ],
      "metadata": {
        "id": "i9c0t45-3zm5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurations"
      ],
      "metadata": {
        "id": "STMNB3Vwz9sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 512\n",
        "\n",
        "batch_sizes = {\n",
        "    'train': 8,\n",
        "    'dev': 4,\n",
        "    'random_test': 4,\n",
        "    'fixed_test': 4,\n",
        "    'fixed_data_deepwd_aug': 4,\n",
        "    'fixed_data_pwws_aug': 4\n",
        "}"
      ],
      "metadata": {
        "id": "5nugq3Lg0V9x"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "XvF9qIrkzds6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d0a6ed-00c6-4cf5-ed8b-a5f2595726b3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op8uCalKzfR4",
        "outputId": "6f3c4f4e-2179-42f2-f3a0-e589a8ac519b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "VZ4SVi9vz3Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/gdrive/MyDrive/NLP_Project\"\n",
        "data = {}\n",
        "\n",
        "for item in ['fixed_test']:\n",
        "  data[item] = pd.read_csv(os.path.join(data_path, item + '.txt'))"
      ],
      "metadata": {
        "id": "sywnjYp-zZmM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "ECRMaaIu0MJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"/content/gdrive/MyDrive/NLP_Project/model_save\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"/content/gdrive/MyDrive/NLP_Project/model_save\")\n",
        "\n",
        "if device.type != 'cpu':\n",
        "  model.cuda()"
      ],
      "metadata": {
        "id": "fVHee2bvziOL"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "dataset = process_data(data, tokenizer, 100, 128, 'end')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj77bOs-6TCp",
        "outputId": "6569249c-f2e3-4559-e43c-01951a0f5d99"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fixed_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = generate_dataloaders(batch_sizes, dataset)"
      ],
      "metadata": {
        "id": "HXYvY9kP5-5Q"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, dataloader in dataloaders.items():\n",
        "  predictions, true_labels = make_predictions(model, dataloader)\n",
        "  predictions = np.concatenate(predictions)\n",
        "  true_labels = np.concatenate(true_labels)\n",
        "  acc, p, r, f1, macro_f1 = evaluate_predictions(predictions, true_labels)"
      ],
      "metadata": {
        "id": "2unRGr0E63iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabe835c-9515-434e-c6e1-a41b60987fa9"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc, f1, macro_f1"
      ],
      "metadata": {
        "id": "vBxojCDyDGIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2659d1-b403-4c0e-efd5-b5d80e892fa6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.44619105199516323,\n",
              " array([0.        , 0.48905109, 0.48484848]),\n",
              " 0.32463319324633194)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__XOzLjSIzT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgMlqA0UIzNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gK8J3GLpIzjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}